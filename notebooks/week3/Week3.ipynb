{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e59838d",
   "metadata": {},
   "source": [
    "# Week 3 — ResNet18 Finetune-All & Light Augmentations (MedMNIST-SSL)\n",
    "\n",
    "This Colab-friendly notebook is a **starter template** for Week 3 of the `medmnist-ssl` project.\n",
    "\n",
    "You will:\n",
    "- Load a **binary** MedMNIST2D dataset (`pneumoniamnist` or `breastmnist`).\n",
    "- Train **ResNet-18** in two regimes:\n",
    "  - `head-only` (frozen backbone, train final classifier layer only)\n",
    "  - `finetune-all` (all layers trainable)\n",
    "- Add **light, modality-aware data augmentations** on the train split.\n",
    "- Collect **per-epoch train/val metrics** and simple **learning curves**.\n",
    "\n",
    "You can later copy the metrics / plots into your repo under\n",
    "`results/week3/<dataset_key>/<your_name>/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you are running on Colab, uncomment the following line.\n",
    "# pip install -q medmnist torch torchvision scikit-learn tqdm\n",
    "\n",
    "echo \"If you're on Colab: uncomment the pip install line above and run this cell once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd65227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Basic config (edit here)\n",
    "# -----------------------------\n",
    "DATASET_KEY = 'pneumoniamnist'  # or 'breastmnist'\n",
    "RUNS_ROOT = './week3_runs'      # where to save metrics and plots\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_HEAD = 5     # for head-only run\n",
    "EPOCHS_ALL = 8      # for finetune-all runs\n",
    "LR = 1e-3\n",
    "USE_IMAGENET_PRETRAINED = True\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.makedirs(RUNS_ROOT, exist_ok=True)\n",
    "print('Using device:', device)\n",
    "print('Saving outputs to:', os.path.abspath(RUNS_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset + transforms\n",
    "# -----------------------------\n",
    "info = INFO[DATASET_KEY]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "n_classes = len(info['label'])\n",
    "print('Dataset:', DATASET_KEY, '| num_classes =', n_classes)\n",
    "\n",
    "# Base normalization (simple 0.5/0.5 for now)\n",
    "MEAN = [0.5, 0.5, 0.5]\n",
    "STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Basic transform (no strong augmentation)\n",
    "base_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(224),\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "# AugA: light geometric augmentation (flip + small rotation)\n",
    "train_transform_augA = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(224),\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "def make_dataloaders(train_aug: str = 'basic'):\n",
    "    \"\"\"Create train/val/test DataLoaders with a chosen train-time augmentation.\n",
    "\n",
    "    train_aug: 'basic' or 'augA'\n",
    "    \"\"\"\n",
    "    if train_aug == 'basic':\n",
    "        train_transform = base_transform\n",
    "    elif train_aug == 'augA':\n",
    "        train_transform = train_transform_augA\n",
    "    else:\n",
    "        raise ValueError(f'Unknown train_aug: {train_aug}')\n",
    "\n",
    "    train_ds = DataClass(split='train', transform=train_transform, download=True)\n",
    "    val_ds   = DataClass(split='val',   transform=base_transform, download=True)\n",
    "    test_ds  = DataClass(split='test',  transform=base_transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ResNet-18 model helper\n",
    "# -----------------------------\n",
    "def create_resnet18(num_classes: int = 2,\n",
    "                    finetune_mode: str = 'head',\n",
    "                    use_imagenet_pretrained: bool = True) -> nn.Module:\n",
    "    \"\"\"Create a ResNet-18 model.\n",
    "\n",
    "    finetune_mode: 'head' (freeze backbone) or 'all' (unfreeze all params)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Newer torchvision API (PyTorch >= 1.12)\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if use_imagenet_pretrained else None\n",
    "        model = models.resnet18(weights=weights)\n",
    "    except AttributeError:\n",
    "        # Fallback to older API\n",
    "        model = models.resnet18(pretrained=use_imagenet_pretrained)\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    if finetune_mode == 'head':\n",
    "        for name, param in model.named_parameters():\n",
    "            if not name.startswith('fc.'):\n",
    "                param.requires_grad = False\n",
    "    elif finetune_mode == 'all':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(f'Unknown finetune_mode: {finetune_mode}')\n",
    "\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f412dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        # MedMNIST labels come as shape [B, 1]\n",
    "        targets = targets.view(-1).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, compute_auroc: bool = True):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.view(-1).long().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            if compute_auroc:\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]  # positive class prob\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "\n",
    "    if compute_auroc:\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_targets, all_probs)\n",
    "        except ValueError:\n",
    "            # If only one class present in y_true\n",
    "            auroc = float('nan')\n",
    "    else:\n",
    "        auroc = float('nan')\n",
    "\n",
    "    return avg_loss, acc, auroc\n",
    "\n",
    "\n",
    "def run_experiment(run_name: str,\n",
    "                  finetune_mode: str,\n",
    "                  train_aug: str,\n",
    "                  num_epochs: int,\n",
    "                  lr: float = 1e-3) -> Dict:\n",
    "    print(f\"\\n=== Run: {run_name} | finetune={finetune_mode} | aug={train_aug} ===\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_dataloaders(train_aug=train_aug)\n",
    "    model = create_resnet18(num_classes=n_classes,\n",
    "                            finetune_mode=finetune_mode,\n",
    "                            use_imagenet_pretrained=USE_IMAGENET_PRETRAINED)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc, _ = evaluate(model, val_loader, criterion, compute_auroc=False)\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"  train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
    "              f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    # Evaluate best model on test set\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    test_loss, test_acc, test_auroc = evaluate(model, test_loader, criterion, compute_auroc=True)\n",
    "    print(f\"Test: loss={test_loss:.4f} | acc={test_acc:.4f} | auroc={test_auroc:.4f}\")\n",
    "\n",
    "    # Save metrics to JSON\n",
    "    out_metrics = {\n",
    "        'run_name': run_name,\n",
    "        'dataset_key': DATASET_KEY,\n",
    "        'finetune_mode': finetune_mode,\n",
    "        'train_aug': train_aug,\n",
    "        'num_epochs': num_epochs,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'test_auroc': test_auroc,\n",
    "        'history': history,\n",
    "    }\n",
    "    metrics_path = os.path.join(RUNS_ROOT, f'metrics_{run_name}.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(out_metrics, f, indent=2)\n",
    "    print('Saved metrics to:', metrics_path)\n",
    "\n",
    "    return out_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run Week 3 experiments\n",
    "# -----------------------------\n",
    "all_runs: List[Dict] = []\n",
    "\n",
    "# 1) Head-only, basic transforms (Week 2 baseline re-run, shorter epochs if needed)\n",
    "run1 = run_experiment(\n",
    "    run_name=f'{DATASET_KEY}_resnet18_head_basic',\n",
    "    finetune_mode='head',\n",
    "    train_aug='basic',\n",
    "    num_epochs=EPOCHS_HEAD,\n",
    "    lr=LR,\n",
    ")\n",
    "all_runs.append(run1)\n",
    "\n",
    "# 2) Finetune-all, basic transforms\n",
    "run2 = run_experiment(\n",
    "    run_name=f'{DATASET_KEY}_resnet18_all_basic',\n",
    "    finetune_mode='all',\n",
    "    train_aug='basic',\n",
    "    num_epochs=EPOCHS_ALL,\n",
    "    lr=LR,\n",
    ")\n",
    "all_runs.append(run2)\n",
    "\n",
    "# 3) Finetune-all, AugA (flip + rotation)\n",
    "run3 = run_experiment(\n",
    "    run_name=f'{DATASET_KEY}_resnet18_all_augA',\n",
    "    finetune_mode='all',\n",
    "    train_aug='augA',\n",
    "    num_epochs=EPOCHS_ALL,\n",
    "    lr=LR,\n",
    ")\n",
    "all_runs.append(run3)\n",
    "\n",
    "print('\\nSummary:')\n",
    "for r in all_runs:\n",
    "    print(r['run_name'], '| acc={:.4f}'.format(r['test_acc']), '| auroc={:.4f}'.format(r['test_auroc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb294c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Plot learning curves\n",
    "# -----------------------------\n",
    "def plot_learning_curves(run_dict: Dict, save_name: str = None):\n",
    "    h = run_dict['history']\n",
    "    epochs = h['epoch']\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, h['train_loss'], label='train_loss')\n",
    "    plt.plot(epochs, h['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(run_dict['run_name'] + ' — loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_name is not None:\n",
    "        path = os.path.join(RUNS_ROOT, save_name)\n",
    "        plt.savefig(path, dpi=150)\n",
    "        print('Saved:', path)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, h['train_acc'], label='train_acc')\n",
    "    plt.plot(epochs, h['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(run_dict['run_name'] + ' — accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_name is not None:\n",
    "        base, ext = os.path.splitext(save_name)\n",
    "        path = os.path.join(RUNS_ROOT, base + '_acc' + ext)\n",
    "        plt.savefig(path, dpi=150)\n",
    "        print('Saved:', path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for r in all_runs:\n",
    "    short_name = r['run_name'].replace(DATASET_KEY + '_', '')\n",
    "    plot_learning_curves(r, save_name=f'curves_{short_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0813f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Move the saved `metrics_*.json` and `curves_*.png` files into your repo under:\n",
    "  `results/week3/<dataset_key>/<your_name>/`.\n",
    "- Use these metrics and curves to fill in **Week 3 tables and plots** in your `README_week3.md`.\n",
    "- Add **dataset-specific observations**:\n",
    "  - For **PneumoniaMNIST**: talk about chest X-ray patterns, false positives/negatives, and which aug helps.\n",
    "  - For **BreastMNIST**: talk about ultrasound noise, subtle lesions, and over/underfitting patterns.\n",
    "\n",
    "This notebook is only a **starter** – you can adjust epochs, learning rates, and augmentation strength\n",
    "as long as you clearly document what you changed in your README."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
